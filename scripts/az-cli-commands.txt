Just showing which commands I ran to migrate the csv datasets to an Azure Blob Storage account. I did this because even after I filtered the original dataset for ratings that were above 7.5 or higher, I still had 76 thousand rows of data. My original plan was to load the modified dataset to an Azure Function to serve as a local cache, but this would potentially overwhelm my Azure Functions as I have the consumption plan enabled. Rather, I decided to migrate the datasets to cloud storage and my functions will obtain relevant datasets when needed. This will keep the functions lightweight with quicker processing times, and keeps the data out of the containerized solution at the end.

To be able to move data into the account, I needed to set up the Storage Account Contributor role for my user profile in the tenant. To ensure the evaluator can access the datasets when the functions are called, I enabled public access to my storage account and blob containers, so no additional permissions should be needed for this.
Note to self: At end of project, disable public access and append sas tokens for evaluators

az login
az account set --subscription <subscriptionID>
az storage container create --name <container_name> --account-name <storage_account_name>
az storage blob upload-batch --account-name <storage_account_name> --destination <container_name> --source <local_path> --pattern "*.csv"


# Generate SAS token
az storage account generate-sas --account-name <storage_account_name> --permissions rl --expiry <expiration> --services b --resource-type o --https-only --output tsv

# Cloud Migration for Azure Function
These Azure Functions were developed locally using Core Tools v4. They're hosted on a consumption plan in Azure, underlying OS is Linux.
Since I had to use the consumption plan for Python v3.9, I had to deploy the functions to Blob Storage as a .zip and then point the Functions resource to the SAS URI.

az storage blob upload --account-name <yourStorageAccount>
  --container-name <yourContainer> \
  --name functionapp.zip \
  --file functionapp.zip \
  --auth-mode key

  az storage blob generate-sas \
  --account-name <yourStorageAccount> \
  --container-name <yourContainer> \
  --name functionapp.zip \
  --permissions r \
  --expiry 2025-12-31T23:59:00Z \
  --auth-mode key \
  --output tsv

  # Setting the configuration as environment variable for Function App 
  az functionapp config appsettings set \
  --name capstone-funcapp \
  --resource-group wgu-capstone \
  --settings WEBSITE_RUN_FROM_PACKAGE="<the-full-zip-URL>"




