# Just showing which commands I ran to migrate the csv datasets to an Azure Blob Storage account. I did this because even after I filtered the original dataset for ratings that were above 7.5 or higher, I still had 76 thousand rows of data. My original plan was to load the modified dataset to an Azure Function to serve as a local cache, but this would potentially overwhelm my Azure Functions as I have the consumption plan enabled. Rather, I decided to migrate the datasets to cloud storage and my functions will obtain relevant datasets when needed. This will keep the functions lightweight with quicker processing times, and keeps the data out of the containerized solution at the end.

# To be able to move data into the account, I needed to set up the Storage Account Contributor role for my user profile in the tenant. To ensure the evaluator can access the datasets when the functions are called, I enabled public access to my storage account and blob containers, so no additional permissions should be needed for this.
# Note to self: At end of project, disable public access and append sas tokens for evaluators

az login
az account set --subscription <subscriptionID>
az storage container create --name <container_name> --account-name <storage_account_name>
az storage blob upload-batch --account-name <storage_account_name> --destination <container_name> --source <local_path> --pattern "*.csv"